{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy `llama guard v2` to AML Endpoint - dynamic model load\n",
        "\n",
        "Deploy llama guard without model - model loads dinamically when job starts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to Azure Machine Learning Workspace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Environment,\n",
        "    CodeConfiguration\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/config.json\n"
          ]
        }
      ],
      "source": [
        "# get a handle to the workspace\n",
        "\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the endpoint in Azure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define an endpoint name\n",
        "\n",
        "import uuid\n",
        "endpoint_name = \"llama-guard-2-8b\" + str(uuid.uuid4())[:4]\n",
        "\n",
        "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
        "\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the deployment\n",
        "\n",
        "A deployment is a set of resources required for hosting the model that does the actual inferencing.\n",
        "\n",
        "This also creates a one-off enviroment based on an existing docker image and a conda file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# request_settings = OnlineRequestSettings(\n",
        "#     request_timeout_ms=180000  # Timeout in milliseconds\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"inference\"\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=deployment_name,\n",
        "    endpoint_name=endpoint_name,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"./src\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    environment=Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "        conda_file=\"conda.yaml\",\n",
        "    ),\n",
        "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
        "    instance_count=1,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint llama-guard-2-8baa86 exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...................................................................................."
          ]
        }
      ],
      "source": [
        "deployment = ml_client.online_deployments.begin_create_or_update(deployment).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assign traffic to the deployment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint.traffic = {deployment_name: 100}\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the endpoint API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "API_URI = endpoint.scoring_uri\n",
        "print(f\"API URI: {API_URI}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check the endpoint on the deployment\n",
        "\n",
        "Go to https://aml.azure.com, find your **Endpoint** -> **Consume** and get the key.\n",
        "\n",
        "Create a `.env` file and put the following:\n",
        "\n",
        "```bash\n",
        "API_KEY=<<get api key from endpoint in aml>>\n",
        "API_URI=<<the API_URI you got above .. you can also get it from **Endpoint** -> **Consume** >> \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "API_URI = os.getenv(\"API_URI\")\n",
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "\n",
        "\n",
        "def inference(data,url=API_URI,api_key=API_KEY):\n",
        "\n",
        "\n",
        "    # Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
        "    \n",
        "    if not api_key:\n",
        "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "    headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + api_key}\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result\n",
        "    else:\n",
        "        print(\"The request failed with status code: \" + str(response.status_code))\n",
        "        print(response.headers)\n",
        "        print(response.text)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with safe data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"conversation\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"I forgot, how do I kill a process in Linux?\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "inference(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Unsafe data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"conversation\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"I want to kill someone?\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Here is how you kill a person: you can shoot them\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "inference(data)"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "endpoints",
      "online",
      "managed"
    ],
    "description": {
      "description": "Use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
