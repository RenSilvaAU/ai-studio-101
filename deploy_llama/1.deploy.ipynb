{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy a Llama model as an AML Endpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "\n",
        "\n",
        "## 1.1. Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    OnlineRequestSettings,\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Configure workspace details and get a handle to the workspace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/config.json\n"
          ]
        }
      ],
      "source": [
        "# get a handle to the workspace\n",
        "\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Define endpoint and deployment\n",
        "\n",
        "## 2.1 Define the endpoint\n",
        "\n",
        "To define an endpoint, you need to specify:\n",
        "\n",
        "* Endpoint name: The name of the endpoint. It must be unique in the Azure region. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n",
        "* Authentication mode: The authentication method for the endpoint. Choose between key-based authentication and Azure Machine Learning token-based authentication. A key doesn't expire, but a token does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n",
        "* Optionally, you can add a description and tags to your endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define an endpoint name\n",
        "\n",
        "\n",
        "import uuid\n",
        "endpoint_name = \"llama-guard-2-8b\" + str(uuid.uuid4())[:4]\n",
        "\n",
        "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
        "\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Define the deployment\n",
        "\n",
        "A deployment is a set of resources required for hosting the model that does the actual inferencing. To deploy a model, you must have:\n",
        "\n",
        "- Model files (or the name and version of a model that's already registered in your workspace). In the example, use use an empty string (no model)\n",
        "- A scoring script, that is, code that executes the model on a given input request. The scoring script receives data submitted to a deployed web service and passes it to the model. The script then executes the model and returns its response to the client. The scoring script is specific to your model and must understand the data that the model expects as input and returns as output. In this example, we have a *score.py* file.\n",
        "- An environment in which your model runs. The environment can be a Docker image with Conda dependencies or a Dockerfile.\n",
        "- Settings to specify the instance type and scaling capacity.\n",
        "\n",
        "The following table describes the key attributes of a deployment:\n",
        "\n",
        "| Attribute      | Description                                                                                                                                                                                                                                                                                                                                                                                    |\n",
        "|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Name           | The name of the deployment.                                                                                                                                                                                                                                                                                                                                                                    |\n",
        "| Endpoint name  | The name of the endpoint to create the deployment under.                                                                                                                                                                                                                                                                                                                                       |\n",
        "| Model          | The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.                                                                                                                                                                                                                                    |\n",
        "| Code path      | The path to the directory on the local development environment that contains all the Python source code for scoring the model. You can use nested directories and packages.                                                                                                                                                                                                                    |\n",
        "| Scoring script | The relative path to the scoring file in the source code directory. This Python code must have an `init()` function and a `run()` function. The `init()` function will be called after the model is created or updated (you can use it to cache the model in memory, for example). The `run()` function is called at every invocation of the endpoint to do the actual scoring and prediction. |\n",
        "| Environment    | The environment to host the model and code. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.                                                                                                                                                                                                                 |\n",
        "| Instance type  | The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).                                                                                                                                                                                                                            |\n",
        "| Instance count | The number of instances to use for the deployment. Base the value on the workload you expect. For high availability, we recommend that you set the value to at least `3`. We reserve an extra 20% for performing upgrades. For more information, see [managed online endpoint quotas](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).                                |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "request_settings = OnlineRequestSettings(\n",
        "    request_timeout_ms=180000  # Timeout in milliseconds\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"inference-3\"\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=deployment_name,\n",
        "    endpoint_name=endpoint_name,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\".\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    request_settings=request_settings,\n",
        "    environment=Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "        conda_file=\"conda.yaml\",\n",
        "    ),\n",
        "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
        "    instance_count=1,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Create local endpoint and deployment\n",
        "\n",
        "## 3.1 Create local endpoint\n",
        "\n",
        "The goal of a local endpoint deployment is to validate and debug your code and configuration before you deploy to Azure. Local deployment has the following limitations:\n",
        "* Local endpoints *do not support* traffic rules, authentication, or probe settings.\n",
        "* Local endpoints support only one deployment per endpoint.\n",
        "* They support local model files only. If you want to test registered models, first download them, then use `path` in the deployment definition to refer to the parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint llama-guard-2-8b5c02 exists\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading deploy_llama (0.03 MBs): 100%|██████████| 30631/30631 [00:00<00:00, 277472.18it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "..............................................................................................."
          ]
        }
      ],
      "source": [
        "deployment = ml_client.online_deployments.begin_create_or_update(deployment).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Create deployment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint.traffic = {deployment_name: 100}\n",
        "endpoint = ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Test the endpoint with sample data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "API_URI = endpoint.scoring_uri\n",
        "print(f\"API URI: {API_URI}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remember to create a .env file with the API key:\n",
        "\n",
        "```bash\n",
        "API_KEY=<<get api key from endpoint in aml>>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "\n",
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "\n",
        "\n",
        "def inference(data,url=API_URI,api_key=API_KEY):\n",
        "\n",
        "\n",
        "    # Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
        "    \n",
        "    if not api_key:\n",
        "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "    headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + api_key}\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result\n",
        "    else:\n",
        "        print(\"The request failed with status code: \" + str(response.status_code))\n",
        "        print(response.headers)\n",
        "        print(response.text)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(inference({\"data\":\n",
        "                      {\"conversation\": \n",
        "                       [{\"user\": \n",
        "                         \"I forgot, how do I kill a process in Linux?\"}, \n",
        "                        {\"assistant\": \n",
        "                         \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"}\n",
        "                        ]}}))"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "endpoints",
      "online",
      "managed"
    ],
    "description": {
      "description": "Use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
