{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy `llama guard v2` to AML Endpoint - with model\n",
    "\n",
    "Deploy llama guard with model - (model is first registered in AML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This model is gated.\n",
    ">\n",
    "> Before you start, please:\n",
    "> - go to [https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B)\n",
    "> - use your Hugging Face login to log in\n",
    "> - request access to the model (it may take a few hours to get it)\n",
    "> - once access has been granted, create a `token`\n",
    "> - create the file `./src/.env` \n",
    "> - add the following to it:\n",
    ">   ```\n",
    ">   HUGGINGFACE_TOKEN=hf_* # your token\n",
    ">   ```\n",
    "> - Now proceed with the instructions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./src/.env\")\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-Guard-2-8B\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/tokenizer_config.json',\n",
       " './model/tokenizer/special_tokens_map.json',\n",
       " './model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('model/model')\n",
    "tokenizer.save_pretrained('./model/tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure Macine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cba\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "\n",
    "print(ml_client.workspace_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/shared/Users/resilv/ai-studio-101/deploy_llama/static_model_load/model' 'https://strensnewerh194762525521.blob.core.windows.net/19f78871-1b4d-4995-a55d-9bb46faef344-azureml-blobstore/LocalUpload/4f87103d13dd33afc2a81f02cec9544a/model' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'Meta-Llama-Guard-2-8B', 'description': './Meta-Llama-Guard-2-8B model', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/providers/Microsoft.MachineLearningServices/workspaces/cba/models/Meta-Llama-Guard-2-8B/versions/3', 'Resource__source_path': '', 'base_path': '/afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/shared/Users/resilv/ai-studio-101/deploy_llama/static_model_load', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f9924d4d420>, 'serialize': <msrest.serialization.Serializer object at 0x7f9924d4cc70>, 'version': '3', 'latest_version': None, 'path': 'azureml://subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/workspaces/cba/datastores/workspaceblobstore/paths/LocalUpload/4f87103d13dd33afc2a81f02cec9544a/model', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': 'Development'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "model = Model(\n",
    "    path=\"./model\",\n",
    "    name=\"Meta-Llama-Guard-2-8B\",\n",
    "    description=\"./Meta-Llama-Guard-2-8B model\"\n",
    ")\n",
    "ml_client.models.create_or_update(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration\n",
    "\n",
    "import uuid\n",
    "endpoint_name =  \"llama-guard-2-8b\"  + str(uuid.uuid4())[:4]\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deployment (the real thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    Environment\n",
    ")\n",
    "deployment_name = \"inference\"\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./src\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "        conda_file=\"conda.yaml\",\n",
    "    ),\n",
    "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint llama-guard-2-8baf7a exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineDeployment({'private_network_connection': None, 'package_model': False, 'provisioning_state': 'Succeeded', 'endpoint_name': 'llama-guard-2-8baf7a', 'type': 'Managed', 'name': 'inference', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/providers/Microsoft.MachineLearningServices/locations/australiaeast/mfeOperationsStatus/odidp:19f78871-1b4d-4995-a55d-9bb46faef344:23c97181-3559-4b9f-8e8e-f4649d0ad348?api-version=2023-04-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/providers/Microsoft.MachineLearningServices/workspaces/cba/onlineEndpoints/llama-guard-2-8baf7a/deployments/inference', 'Resource__source_path': '', 'base_path': '/afh/projects/cba-19f78871-1b4d-4995-a55d-9bb46faef344/shared/Users/resilv/ai-studio-101/deploy_llama/static_model_load', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f9924aa7f70>, 'model': '/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/providers/Microsoft.MachineLearningServices/workspaces/cba/models/Meta-Llama-Guard-2-8B/versions/3', 'code_configuration': {'code': '/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/providers/Microsoft.MachineLearningServices/workspaces/cba/codes/f3ebea13-eb62-4818-b51b-1508f3d7cb74/versions/1'}, 'environment': '/subscriptions/691b572d-8686-481a-9757-4befaa7f9526/resourceGroups/rg-resilvai/providers/Microsoft.MachineLearningServices/workspaces/cba/environments/CliV2AnonymousEnvironment/versions/76dc897747211a956bb877f3b4d3b153', 'environment_variables': {'AZUREML_MODEL_DIR': '/var/azureml-app/azureml-models/Meta-Llama-Guard-2-8B/3', 'AZUREML_ENTRY_SCRIPT': 'score.py', 'AML_APP_ROOT': '/var/azureml-app/src'}, 'app_insights_enabled': False, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7f9926751480>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7f9926750cd0>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f99267513c0>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f9924aa7d30>, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_NC24ads_A100_v4', 'data_collector': None, 'egress_public_network_access': 'Enabled'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Traffic to deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    }
   ],
   "source": [
    "endpoint.traffic = {deployment_name: 100}\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the endpoint URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API URI: https://llama-guard-2-8baf7a.australiaeast.inference.ml.azure.com/score\n"
     ]
    }
   ],
   "source": [
    "API_URI = endpoint.scoring_uri\n",
    "print(f\"API URI: {API_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the endpoint on the deployment\n",
    "\n",
    "Go to https://aml.azure.com, find your **Endpoint** -> **Consume** and get the key.\n",
    "\n",
    "Create a `.env` file and put the following:\n",
    "\n",
    "```bash\n",
    "API_KEY=<<get api key from endpoint in aml>>\n",
    "API_URI=<<the API_URI you got above .. you can also get it from **Endpoint** -> **Consume** >> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "\n",
    "\n",
    "API_URI = os.getenv(\"API_URI\")\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "def inference(data,url=API_URI,api_key=API_KEY):\n",
    "\n",
    "\n",
    "    # Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "    \n",
    "    if not api_key:\n",
    "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "    headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + api_key}\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result\n",
    "    else:\n",
    "        print(\"The request failed with status code: \" + str(response.status_code))\n",
    "        print(response.headers)\n",
    "        print(response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "  \"conversation\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I forgot, how do I kill a process in Linux?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "inference(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Unsafe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe\\nS1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "  \"conversation\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to kill someone?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Here is how you kill a person: you can shoot them\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "inference(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
