{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy `llama guard v2` to AML Endpoint - with model\n",
    "\n",
    "Deploy llama guard with model - (model is first registered in AML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This model is gated.\n",
    ">\n",
    "> Before you start, please:\n",
    "> - go to [https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B)\n",
    "> - use your Hugging Face login to log in\n",
    "> - request access to the model (it may take a few hours to get it)\n",
    "> - once access has been granted, create a `token`\n",
    "> - create the file `./src/.env` \n",
    "> - add the following to it:\n",
    ">   ```\n",
    ">   HUGGINGFACE_TOKEN=hf_* # your token\n",
    ">   ```\n",
    "> - Now proceed with the instructions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./src/.env\")\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-Guard-2-8B\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model/model')\n",
    "tokenizer.save_pretrained('./model/tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure Macine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "\n",
    "print(ml_client.workspace_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "model = Model(\n",
    "    path=\"./model\",\n",
    "    name=\"Meta-Llama-Guard-2-8B\",\n",
    "    description=\"./Meta-Llama-Guard-2-8B model\"\n",
    ")\n",
    "ml_client.models.create_or_update(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration\n",
    "\n",
    "import uuid\n",
    "endpoint_name =  \"llama-guard-2-8b\"  + str(uuid.uuid4())[:4]\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deployment (the real thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    Environment\n",
    ")\n",
    "deployment_name = \"inference\"\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./src\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "        conda_file=\"conda.yaml\",\n",
    "    ),\n",
    "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Traffic to deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {deployment_name: 100}\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the endpoint URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URI = endpoint.scoring_uri\n",
    "print(f\"API URI: {API_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the endpoint on the deployment\n",
    "\n",
    "Go to https://aml.azure.com, find your **Endpoint** -> **Consume** and get the key.\n",
    "\n",
    "Create a `.env` file and put the following:\n",
    "\n",
    "```bash\n",
    "API_KEY=<<get api key from endpoint in aml>>\n",
    "API_URI=<<the API_URI you got above .. you can also get it from **Endpoint** -> **Consume** >> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "\n",
    "\n",
    "API_URI = os.getenv(\"API_URI\")\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "def inference(data,url=API_URI,api_key=API_KEY):\n",
    "\n",
    "\n",
    "    # Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "    \n",
    "    if not api_key:\n",
    "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "    headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + api_key}\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result\n",
    "    else:\n",
    "        print(\"The request failed with status code: \" + str(response.status_code))\n",
    "        print(response.headers)\n",
    "        print(response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"conversation\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I forgot, how do I kill a process in Linux?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "inference(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Unsafe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"conversation\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to kill someone?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Here is how you kill a person: you can shoot them\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "inference(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
